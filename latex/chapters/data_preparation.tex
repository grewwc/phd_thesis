\chapter{Data Preparation}
  As previously mentioned, we use \textit{Kepler} DR24 data 
  \footnote{ https://exoplanetarchive.ipac.caltech.edu/docs/Kepler\_TCE\_docs.html}
  with labels to train out model. It is a CSV file while other types are also 
  supported. Note that the data contains training labels generated by autovetter which 
  is basically a
  random forest technique. Random forest is also a supervised machine learning algorithm,
  thus it also needs labels to tell the algorithm which one is PC. The labels fed into 
  random forest algorithm are classified by humans. The reason why use deep learning method 
  to redo the classification task is because deep learning performs very well when trained 
  with large amount of data. Therefore, we try to use the human-made and normal machine 
  learning method generated labels to train a deep learning model in order to get better 
  accuracy.

  The labels contain four unique values: 
  PC (Planet Candidates) , AFP (Astronomical False Positive), NTP 
  (Non-transiting Phenomenon) and UNK (Unknown). There are 27 columns in the data including 
  the label column named "av\_training\_set". Each other column represents a property of one 
  TCE and some of the columns are more important for our classification task, such as 
  "tce\_period", "tce\_duration", "tce\_prad", "tce\_depth", etc. Meanings of the some 
  column names are listed in the following table.
  \footnote{https://exoplanetarchive.ipac.caltech.edu/docs/API\_tce\_columns.html}

  \begin{table}[!htp]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{|c|c|} 
      \hline 
      \textbf{Column Name} & \textbf{Definition} \\
      \hline 
      tce\_period & time interval between two consecutive transits in days \\
      \hline 
      tce\_depth & starlight drops in ppm \\
      \hline 
      tce\_prad & the planet radius in Earth Radii \\
      \hline 
    \end{tabular}}
    \centering
    \caption[Descriptions of column names.]{Descriptions of column names. The listed 
      properties are more important for identifying PC.}
    \label{table: difinitions_of_column_names}
  \end{table}
  
  The CSV table needs to be preprocessed. First of all, we need to drop all data labeled with 
  'UNK'. Then since deep learning algorithms only deal with numerical labels, we need to 
  label PC and non-PC as 1 and 0 respectively. Non-PC includes both AFP and NTP because we do 
  binary classification to find out planet candidates. After processing the basic properties 
  for each TCE, we also need to download their light curves.

  \section{Download Light Curves}
    Light curves are downloaded based on the column name "kepid" in the \textit{Kepler} DR24 
    data table previously discussed. All the light curves can be found on the website 
    \blackhref{https://archive.stsci.edu/pub/kepler/lightcurves/0020/}
    {https://archive.stsci.edu/pub/kepler/lightcurves/0020/}. However, there are 20367 TCE
    in the table containing more than 330,000 light curve files. Downloading these 
    files can be very time consuming (about 2 weeks by estimation). Therefore, I scrape the 
    webpage in parallel to filter the light curves data and download them. 

    Take kepid 000757137 as an example. The URL of the light curves for this TCE is 
    \blackhref{http://archive.stsci.edu/pub/kepler/lightcurves/0007/000757137}
    {http://archive.stsci.edu/pub/kepler/lightcurves/0007/000757137}. There are 17 light 
    curve files as the following screen shot shown.
    \begin{figure}[!htp]
      \centering
      \includegraphics[scale=0.4]{kepid_light_curves_example.PNG}
      \caption{The screen shot of webpage containing light curves of kepid 000757137}
      \label{fig: kepid_light_curves_example}
    \end{figure}

    We can find the hyper link from the anchor tag and navigate to the URL of the desired 
    data. The code is written in Golang, which is very good at concurrency. All code can be 
    found on \blackhref{https://github.com/grewwc/exoplanet}{github}.
    In short, the basic procedure is 
    \begin{enumerate}
      \item Get all the hyper links from the root URL 
        \blackhref{http://archive.stsci.edu/pub/kepler/lightcurves/}
        {http://archive.stsci.edu/pub/kepler/lightcurves/}. Join the current URL with 
        the parsed hyperlinks to generate the new URL. 
      \item Launch a new goroutine to search from that webpage.
      \item Get the hyper links from the child URL. If the hyper links are fits files, then 
        download them to a local directory. Otherwise, as the procedure 1, get the new 
        URL and launch a new goroutine to do the search recursively.
    \end{enumerate}
  