\chapter{Data Preparation}
  As previously mentioned, we use \textit{Kepler} DR24 data 
  \footnote{ https://exoplanetarchive.ipac.caltech.edu/docs/Kepler\_TCE\_docs.html}
  with labels to train out model. It is a CSV file while other types are also 
  supported. Note that the data contains training labels generated by autovetter which 
  is basically a
  random forest technique. Random forest is also a supervised machine learning algorithm,
  thus it also needs labels to tell the algorithm which one is PC. The labels fed into 
  random forest algorithm are classified by humans. The reason why use deep learning method 
  to redo the classification task is because deep learning performs very well when trained 
  with large amount of data. Therefore, we try to use the human-made and normal machine 
  learning method generated labels to train a deep learning model in order to get better 
  accuracy.

  The labels contain four unique values: 
  PC (Planet Candidates) , AFP (Astronomical False Positive), NTP 
  (Non-transiting Phenomenon) and UNK (Unknown). There are 27 columns in the data including 
  the label column named "av\_training\_set". Each other column represents a property of one 
  TCE and some of the columns are more important for our classification task, such as 
  "tce\_period", "tce\_duration", "tce\_prad", "tce\_depth", etc. Meanings of the some 
  column names are listed in the following table.
  \footnote{https://exoplanetarchive.ipac.caltech.edu/docs/API\_tce\_columns.html}

  \begin{table}[!htp]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{|c|c|} 
      \hline 
      \textbf{Column Name} & \textbf{Definition} \\
      \hline 
      tce\_period & time interval between two consecutive transits in days \\
      \hline 
      tce\_depth & starlight drops in ppm \\
      \hline 
      tce\_prad & the planet radius in Earth Radii \\
      \hline 
    \end{tabular}}
    \centering
    \caption[Descriptions of column names.]{Descriptions of column names. The listed 
      properties are more important for identifying PC.}
    \label{table: difinitions_of_column_names}
  \end{table}
  
  The CSV table needs to be preprocessed. First of all, we need to drop all data labeled with 
  'UNK'. Then since deep learning algorithms only deal with numerical labels, we need to 
  label PC and non-PC as 1 and 0 respectively. Non-PC includes both AFP and NTP because we do 
  binary classification to find out planet candidates. After processing the basic properties 
  for each TCE, we also need to download their light curves.

  \section{Download Light Curves}
    Light curves are downloaded based on the column name "kepid" in the \textit{Kepler} DR24 
    data table previously discussed. There are about 16 fits files which is composed of four 
    year observation corresponding to a kepid. All the light curves can be found on the 
    website \blackhref{https://archive.stsci.edu/pub/kepler/lightcurves/0020/}
    {https://archive.stsci.edu/pub/kepler/lightcurves/0020/}. A tricky thing is that 
    there are 20367 TCE
    in the table containing more than 330,000 light curve files. Downloading these 
    files can be very time consuming (about 2 weeks by estimation). Therefore, I scrape the 
    webpage in parallel to filter the light curves data and download them. 

    Take kepid 000757137 as an example. The URL of the light curves for this TCE is 
    \blackhref{http://archive.stsci.edu/pub/kepler/lightcurves/0007/000757137}
    {http://archive.stsci.edu/pub/kepler/lightcurves/0007/000757137}. There are 17 light 
    curve files as the following screen shot shown.
    \begin{figure}[!htp]
      \centering
      \includegraphics[scale=0.4]{kepid_light_curves_example.PNG}
      \caption{The screen shot of webpage containing light curves of kepid 000757137}
      \label{fig: kepid_light_curves_example}
    \end{figure}

    We can find the hyper link from the anchor tag and navigate to the URL of the desired 
    data. The code is written in Golang, which is very good at concurrency. All code can be 
    found on \blackhref{https://github.com/grewwc/exoplanet}{github}.
    In short, the basic procedure is 
    \begin{enumerate}
      \item Parse all the hyperlinks from the root URL: \\
        \blackhref{http://archive.stsci.edu/pub/kepler/lightcurves}
        {http://archive.stsci.edu/pub/kepler/lightcurves}. Join the current URL with 
        the parsed hyperlinks to generate new URLs. 
      \item Launch new goroutine to search from the newly generated URLs.
      \item Get the hyper links from the child URLs. If the hyper links are fits files, 
        then download them to a local directory in parallel. 
        Otherwise, as the procedure 1, get new 
        URLs and launch new goroutine to do the search recursively.
    \end{enumerate}

    Another problem is if I search from too many URL simultaneously, my IP will be blocked
    temporarily. Therefore, I need to control the total amount of processes. After some 
    attempts, I find that 200 processes are good enough and all light curves can be 
    downloaded within 3 hours. The point is data are usually very large and complex 
    in the field of astronomy, thus dealing with data efficiently is important.

    
  \section{Light Curves Pre-processing}
    As previously discussed, the depths of TCE are in general very small and the median 
    value is around 125 ppm. The following figure is the four-year observation data. 
    Thus we need to fold the light curves and bin them in order to magnify the effect.
    Moreover, we can see some data points in the top right side of the figure. We also 
    need to remove the outliers to improve out model's prediction accuracy.
    
    \begin{figure}[!htp]
      \centering
      \includegraphics[scale=0.6]{11442793_time_flux.png}
      \caption[Plot of normalized flux to time of kepid 11442793.]
        {Plot of normalized flux to time of kepid 11442793. The flux is normalized by 
        dividing the original flux by their median value.}
      \label{fig: 11442793_time_flux}
    \end{figure}

    In addition, there is long-term trend which is clearly not a transit signal
    in the light curves for some kepids. For example, we can see a clear periodical 
    fluctuation in the light curve for kepid 1164109 as the following figure shown.
    The fluctuation has bad effects for the deep learning model, thus even though we don't 
    have to deal with the long-term trend, removing the trend may increase the model's 
    performance.

    \begin{figure}[!htp]
      \centering
      \includegraphics[scale=0.5]{1164109_time_flux.png}
      \caption[Plot of normalized flux to time of kepid 1164109.]
        {Plot of normalized flux to time of kepid 1164109.}
      \label{fig: 1164109_time_flux}
    \end{figure}

    Finally, since there may be more than one planet associated with a kepid, we remove 
    signals of other planets in the same system to avoid interference. The complete 
    progress of light curves preprocessing is described as follows.

    \subsection{Remove Multiple Signals in the Same Stellar System}
      The following table shows some useful parameters of TCE for kepid 1162345. 
      When preprocessing kepid 1162345, we fetch all the TCE --- there are 3 TCE in the stellar 
      system. Then we process the TCE one by one, with other two TCE removed. The remove logic is simple:
      mask out the data points within the range [center-duration, center+duration] and return the new 
      flux and time pairs. 

      \begin{table}[!htp]
        \centering
        \scalebox{0.8}{
        \begin{tabular}{|c|c|c|c|} 
          \hline 
          \textbf{tce\_plnt\_number} & \textbf{tce\_period (day)} & \textbf{tce\_duration (hr)} & \textbf{tce\_depth (ppm)} \\
          \hline 
          2	& 0.831850 & 2.392 & 2.636\\
          \hline 
          3	& 0.831833 & 2.181 & 27.100\\
          \hline 
          1	& 0.831777 & 2.349 & 24.270	\\ 
          \hline 
        \end{tabular}}
        \centering
        \caption[Parameters of three TCE for kepid 1162345.]
          {Parameters of three TCE for kepid 1162345. When processing the TCE for planet 2, we remove the signals for 
          planet 1 and 3.}
        \label{table: params_1162345}
      \end{table}
    
    \subsection{Remove Outliers and Long-term Trend}
      Before doing further processing to the light curves, we need to remove the outliers 
      first to increase our model's performance. We just use a simple sigma clip to do the 
      job. The method firstly derive the median and standard deviation value
      of the flux ($m, \sigma$). Then any points not within the range of $m \pm n\sigma$ 
      are removed, where n is a parameter with default value of 3.

      Then we should remove the long-term trend in the light curves. 
      In this thesis, we use a python package called lightkurve 
      \footnote{https://docs.lightkurve.org/} which internally uses 
      Savizky-Golay filter to remove the low-frequency trend. The lightkurve package 
      contains an python object called LightKurve which has a "flatten" method. After 
      some experiments, we change some default parameter values as the following table shown:

      \begin{table}[!htp]
        \centering
        \scalebox{0.8}{
        \begin{tabular}{|c|c|c|c|c|} 
          \hline 
           & \textbf{window\_length} & \textbf{polyorder} & \textbf{break\_tolerance} & \textbf{sigma} \\
          \hline 
          \textbf{values} & 201 & 2 & 40 & 3 \\
          \hline
        \end{tabular}}
        \centering
        \caption[Parameter values chosen in the LightKurve flatten method.]
          {Parameter values chosen in the LightKurve flatten method. The detailed 
          descriptions of these parameters can be found on the lightkurve official 
          website.}
        \label{table: params_1162345}
      \end{table}

      Take kepid 1164109 for example, the following figures are original normalized flux 
      and flattened flux. After removing the long-term trend from the original normalized 
      light curve, the transit signal becomes more clear. 

      \begin{figure}[!ht]  
        \begin{center}
        \begin{minipage}{0.45\textwidth}
          \begin{center} 
              \includegraphics[scale=0.35]{original__1164109.png}
          \end{center}
        \end{minipage}
        \begin{minipage}{0.45\textwidth}
          \begin{center} 
              \includegraphics[scale=0.35]{flattened_1164109.png}
          \end{center}
        \end{minipage}
      \end{center}
      \begin{center}
        \caption[Comparison between original light curve and flattened light curve.]
          {Comparison between original light curve and flattened light curve. The right 
          panel is the normalized light curve of kepid 1164109 after flattening. }
        \label{fig: 1164109_comparison}
        \end{center}
      \end{figure}

    \subsection{Fold and Bin Light Curves}
      First of all, we fold the flattened light curves to increase the data points within 
      the range of the transit signals. Then we move the transit signal in the center 
      of the folded light curve. This may increase the performance of the deep learning model 
      since the data is more "clean". Then we need to bin the folded light curves for the 
      following benefits. 
      \begin{itemize}
        \item There are too many data points in the folded light curves 
          (around 65000 for each folded light curve). 
          As we know, the number of "useful" data points which are within the range of a 
          TCE's duration is very small. Therefore, most of the 65000 data points are 
          nearly "useless" and will waste a lot of computing power. 
        \item Binning can make the light curves a bit more smoother and reduce variants.
          Since a complex machine learning model tend to fit the variants and causes
          over-fitting, which is very bad for the model generalization. Thus, the performance
          of the over-fitted model tend to perform bad on the unseen data.
      \end{itemize}

      There are some python packages such as lightkurve to do the binning operation, however, 
      we can only choose the bin width ($w$) or number of bins. And the distance between 
      each bin ($d$) is equal to the bin width ($d=w$). But these two values can be different:
      if $d<w$, the binned light curves can be more smooth and may can improve the model's 
      performance. Therefore, we need to write our own bin method. Even though the logic 
      of binning method is simple, it is a little bit tricky to implement. For example, if the bin width is so small that there is no data points in this range, what value 
      should we choose to 
      represent this bin? In this thesis, we choose the median value if the bin has no 
      data points. Another question is should we choose the same 
      bin width and distance for all the light curves? After some experiments, 
      we don't find a 
      good criteria to tune the binning parameters for each light curve. Therefore, we use 
      the same parameter configuration for all the light curves. Finally, the performance for 
      this method is very important because we need to do 
      many training experiments, thus we have to generate preprocessed light curves for all 
      the TCE multiple times. 

      As mentioned previously, the TCE duration is very small compare to the period. 
      Take the TCE with kepid 1164109 for example, the period is around 622 days while 
      the period is only 12 hours. Thus we would like to focus more on the data points within 
      the TCE durations and have more bins in this range. In short, the binning logic is the 
      following. We divide a light curve into three parts: before transit, during and after 
      transit. Firstly, we pass the duration as a parameter to the bin function. Because 
      we have already arranged the transit signals in the center of light curves, 
      we choose the data points within the range [-duration, +duration] around the center.
      Then we count the number of the data points ($n$), and the number of bins in the 
      transit range is ($n/5$). If we denote the 
      total desired number of bins (the input shape of the model) as $n\_total$.
      For the other two parts, the number of bins are the same, which equals to 
      $\left(n\_total-n/5\right)/2$. 
      

      
